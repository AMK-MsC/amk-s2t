{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30799df1-3a27-49c8-bf5f-1007b459b8ee",
   "metadata": {},
   "source": [
    "# Load local dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f4a0a2-c90e-4e8b-9982-250f33a33e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = \"amk-whisper-v5.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4a5b0c-0c10-476f-b216-da3b58d9194f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8260caec-a8bc-436e-839d-d020bea09354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f0a0ee70fe45b6b54583c1dac75da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfcea90-9fca-4d5c-924a-d4fddc44816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c777b1-73a1-4586-9f1c-9392e3163376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce147ed30d7457f8a58f62347341f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration 113-sim-dataset-a5724a57e874f136\n",
      "Found cached dataset audiofolder (/home/amk/.cache/huggingface/datasets/audiofolder/113-sim-dataset-a5724a57e874f136/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"./data/113-sim-dataset/\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da84e402-8318-4aa7-a5e5-1ed3387302ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part0.wav',\n",
       "  'array': array([ 0.0007019 ,  0.0007019 ,  0.0007019 , ...,  0.04537964,\n",
       "          0.        , -0.00387573], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part1.wav',\n",
       "  'array': array([ 0.02828979, -0.02804565, -0.02658081, ...,  0.9076843 ,\n",
       "          0.70578   ,  0.4340515 ], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part10.wav',\n",
       "  'array': array([ 0.03292847,  0.01608276, -0.01119995, ..., -0.00094604,\n",
       "         -0.00094604, -0.00094604], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part11.wav',\n",
       "  'array': array([ 0.        ,  0.00119019,  0.00143433, ..., -0.00021362,\n",
       "         -0.00021362, -0.00045776], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part12.wav',\n",
       "  'array': array([-0.00021362, -0.00021362,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part13.wav',\n",
       "  'array': array([ 0.        ,  0.        ,  0.00021362, ..., -0.00021362,\n",
       "          0.00045776,  0.        ], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part14.wav',\n",
       "  'array': array([0.        , 0.00021362, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part2.wav',\n",
       "  'array': array([-0.0319519 , -0.56051636, -0.8908386 , ...,  0.00875854,\n",
       "          0.0072937 ,  0.00460815], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part3.wav',\n",
       "  'array': array([ 0.00216675,  0.00021362, -0.00192261, ...,  0.        ,\n",
       "         -0.00021362, -0.00021362], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part4.wav',\n",
       "  'array': array([-0.00021362, -0.00021362,  0.        , ..., -0.00045776,\n",
       "         -0.00021362, -0.00021362], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part5.wav',\n",
       "  'array': array([ 0.        ,  0.00021362,  0.        , ..., -0.0007019 ,\n",
       "          0.        ,  0.00045776], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part6.wav',\n",
       "  'array': array([ 0.0007019 ,  0.00094604,  0.00119019, ...,  0.        ,\n",
       "         -0.00582886,  0.00021362], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part7.wav',\n",
       "  'array': array([-0.00436401, -0.00436401, -0.01119995, ..., -0.00045776,\n",
       "          0.        ,  0.        ], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part8.wav',\n",
       "  'array': array([ 0.        , -0.00045776,  0.        , ..., -0.00411987,\n",
       "          0.01754761,  0.02487183], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part9.wav',\n",
       "  'array': array([ 0.01535034,  0.00460815, -0.00021362, ...,  0.09494019,\n",
       "          0.06344604,  0.05026245], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/Emil0_VA(I).wav',\n",
       "  'array': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "         -9.1552734e-05, -6.1035156e-05, -3.0517578e-05], dtype=float32),\n",
       "  'sampling_rate': 48000},\n",
       " {'path': '/home/amk/amk/data/113-sim-dataset/audio/Emil4_VA(i).wav',\n",
       "  'array': array([-0.00018311, -0.00045776, -0.00033569, ...,  0.00204468,\n",
       "          0.0015564 ,  0.00134277], dtype=float32),\n",
       "  'sampling_rate': 48000}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2e1ba2-9451-4991-aa5a-f63cbd8b6b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/amk/.cache/huggingface/datasets/audiofolder/113-sim-dataset-a5724a57e874f136/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc/cache-6a6560642171925c.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1bfefe7-cd3e-4dab-9a83-fdd565cda791",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0b0a66-bbf5-4324-ab0b-d49d2d0cac63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'start', 'stop', 'transcription'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'start', 'stop', 'transcription'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d90ac99-7fe9-4ffe-ac29-53d95162ee3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/amk/amk/data/113-sim-dataset/audio/113-samtale_part7.wav',\n",
       "  'array': array([-0.00436401, -0.00436401, -0.01119995, ..., -0.00045776,\n",
       "          0.        ,  0.        ], dtype=float32),\n",
       "  'sampling_rate': 8000},\n",
       " 'start': 171,\n",
       " 'stop': 191,\n",
       " 'transcription': 'Nå ligger hun på siden. Sjekk om hun fortsatt. Eva! Nå må du sjekke om hun. Hva? Hva sa du? Hva sier du?  Puster normalt. Hva sa du? Sjekk om hun fortsatt puster normalt. Ja, ja. Ja gjør hun det? Ja, hun puster fortsatt normalt. Ja.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc64cf0-9aa8-46d7-a9fa-c95b2b56cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3372c4e5-c2c2-4579-8f1d-f2ad6eac3ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Jethuestad/amk-whisper-v5.5/commit/8219bc631ef4c335fbb7a29982da87d023a65c2b', commit_message='Upload tokenizer', commit_description='', oid='8219bc631ef4c335fbb7a29982da87d023a65c2b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v2\", language=\"norwegian\", task=\"transcribe\")\n",
    "tokenizer.push_to_hub(REPO_NAME, private=True, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a108349b-3842-4880-9fca-2b5820ee1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\", language=\"norwegian\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d22646-81ec-469a-bd8e-448d93982c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 Nå ligger hun på siden. Sjekk om hun fortsatt. Eva! Nå må du sjekke om hun. Hva? Hva sa du? Hva sier du?  Puster normalt. Hva sa du? Sjekk om hun fortsatt puster normalt. Ja, ja. Ja gjør hun det? Ja, hun puster fortsatt normalt. Ja.\n",
      "Decoded w/ special:    <|startoftranscript|><|no|><|transcribe|><|notimestamps|>Nå ligger hun på siden. Sjekk om hun fortsatt. Eva! Nå må du sjekke om hun. Hva? Hva sa du? Hva sier du?  Puster normalt. Hva sa du? Sjekk om hun fortsatt puster normalt. Ja, ja. Ja gjør hun det? Ja, hun puster fortsatt normalt. Ja.<|endoftext|>\n",
      "Decoded w/out special: Nå ligger hun på siden. Sjekk om hun fortsatt. Eva! Nå må du sjekke om hun. Hva? Hva sa du? Hva sier du?  Puster normalt. Hva sa du? Sjekk om hun fortsatt puster normalt. Ja, ja. Ja gjør hun det? Ja, hun puster fortsatt normalt. Ja.\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = dataset[\"train\"][0][\"transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c1efa52-d0b7-4107-b8d5-6408f9caf9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3008dcf1-c54e-4dcb-81b0-64b7d5a76378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/amk/amk/data/113-sim-dataset/audio/Emil4_VA(i).wav',\n",
       "  'array': array([-0.00022369, -0.00022327, -0.00073793, ...,  0.00182292,\n",
       "          0.00216485,  0.        ], dtype=float32),\n",
       "  'sampling_rate': 16000},\n",
       " 'start': 0,\n",
       " 'stop': 29,\n",
       " 'transcription': 'Sånn. Nå ligger hun ned på siden. Puster hun fortsatt normalt? Hæ? Hva sa du? Puster hun fortsatt normalt? Ja, ja hun puster fortsatt normalt. Nå må du komme snart da, vær så snill. Ja, de er snart til hjelp hos dere. Ja, jeg trenger hjelp. Jeg klarer ikke dette her alene. Jeg vet ikke hva jeg skal gjøre. Du klarer deg veldig fint nå. Kommer det til å gå bra? Det er veldig bra at hun er litt våken nå. Ja. Og at hun puster normalt. Ja, ja. Når er Eva født?'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e44ba2a-d1d4-4173-9b73-e4c174be9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 8 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97ca3b0f-46bf-4d4b-8ad0-b6ac61311a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c967980a15045e096207980392fbcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3373fba78aad42cb83b2444e1fbcb8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/3 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e20aa4daef40e3a9c6bf443a9f3d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/3 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5598c0eb17624ea6a7d93d37237bd9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/4 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146c03953cb945908d6eee2d835b3acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9eefcf7e3141bf8a7296bfdc0b5a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4d5854760442819993f818cc77edf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e23535a552e4bba84155dc2be542284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d807fde-f7e3-446b-9a73-0e4777747d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e290d14b-addb-4531-9a79-e63f32ee0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e3e8dd7-ede4-4504-b708-5fec8c6e74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd9c364a-c878-4d30-a664-b087b6b17b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59f40678-317b-435e-a787-ec4bc200adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70eca947-dcc0-4f74-b5fd-f030e33b095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c01b8306-d36d-419e-9201-10d97f195baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=REPO_NAME,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=5e-6,\n",
    "    warmup_steps=2,\n",
    "    #max_steps=100,\n",
    "    num_train_epochs=10,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    #save_steps=20,\n",
    "    #eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8f583ea-d9df-4332-8a38-e305f09bdede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/Jethuestad/amk-whisper-v5.5 into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59ade559-7124-47d4-94a2-9e4e5598b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amk/anaconda3/envs/amk/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 04:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.293500</td>\n",
       "      <td>1.084155</td>\n",
       "      <td>38.265306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.293500</td>\n",
       "      <td>1.033903</td>\n",
       "      <td>35.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.181100</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>33.163265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.995300</td>\n",
       "      <td>0.910253</td>\n",
       "      <td>28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.799400</td>\n",
       "      <td>0.885159</td>\n",
       "      <td>28.061224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>0.862435</td>\n",
       "      <td>28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.842288</td>\n",
       "      <td>28.826531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.827379</td>\n",
       "      <td>28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.557200</td>\n",
       "      <td>0.810995</td>\n",
       "      <td>27.806122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.804855</td>\n",
       "      <td>28.061224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.860892379283905, metrics={'train_runtime': 263.9318, 'train_samples_per_second': 0.493, 'train_steps_per_second': 0.038, 'total_flos': 2.76012232704e+17, 'train_loss': 0.860892379283905, 'epoch': 10.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "796a76b5-0d2b-43e5-aa35-ad55c1a51269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c432cef73d43bd9398ced0873f3aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/5.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Jethuestad/amk-whisper-v5.5\n",
      "   8219bc6..4eac23a  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95339fb1-12b9-4a7e-bb0f-e4cd6b10f8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amk",
   "language": "python",
   "name": "amk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
